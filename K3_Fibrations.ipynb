{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35197596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        row = line.strip().split()\n",
    "        data.append(row)\n",
    "\n",
    "    max_cols = max(len(row) for row in data)\n",
    "    cols = [f\"col{i}\" for i in range(1, max_cols+1)]\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "    return df\n",
    "\n",
    "# Import the data\n",
    "df = read_data('C:\\\\Users\\\\aaron\\\\Desktop\\\\CY_Manifolds\\\\Hodge.K3\\\\Hodge.K3')\n",
    "\n",
    "# Name the import columns\n",
    "df = df.rename(columns={'col1': 'w1'})\n",
    "df = df.rename(columns={'col2': 'w2'})\n",
    "df = df.rename(columns={'col3': 'w3'})\n",
    "df = df.rename(columns={'col4': 'w4'})\n",
    "df = df.rename(columns={'col5': 'w5'})\n",
    "df = df.rename(columns={'col6': 'd'})\n",
    "df = df.rename(columns={'col7': 'TS'})\n",
    "df = df.rename(columns={'col8': 'h11'}) # Hodge number\n",
    "df = df.rename(columns={'col9': 'h12'}) # Hodge number\n",
    "df = df.rename(columns={'col10': 'hashP'})\n",
    "df = df.rename(columns={'col11': 'hashV'})\n",
    "df = df.rename(columns={'col12': 'hashPbar'})\n",
    "df = df.rename(columns={'col13': 'hashVbar'})\n",
    "df = df.rename(columns={'col14': 'PI'}) # K3 Projection\n",
    "df = df.rename(columns={'col15': 'facet0'}) # K3 Facets\n",
    "df = df.rename(columns={'col16': 'facet1'}) # K3 Facets\n",
    "df = df.rename(columns={'col17': 'facet2'}) # K3 Facet\n",
    "\n",
    "# Remove the first two characters from the 'Column1' column\n",
    "df['d'] = df['d'].str.slice(0, -2)\n",
    "df['h11'] = df['h11'].str.slice(2)\n",
    "df['hashP'] = df['hashP'].str.slice(2)\n",
    "df['hashPbar'] = df['hashPbar'].str.slice(2)\n",
    "df['PI'] = df['PI'].str.slice(2)\n",
    "df['facet0'] = df['facet0'].str.slice(2)\n",
    "\n",
    "# Conversion from string to integer\n",
    "df['w1'] = df['w1'].astype(int) # weight 1\n",
    "df['w2'] = df['w2'].astype(int) # weight 2\n",
    "df['w3'] = df['w3'].astype(int) # weight 3\n",
    "df['w4'] = df['w4'].astype(int) # weight 4\n",
    "df['w5'] = df['w5'].astype(int) # weight 5\n",
    "df['d'] = df['d'].astype(int) # degree - sum of the weights\n",
    "df['h11'] = df['h11'].astype(int) # hodge number h11\n",
    "df['h12'] = df['h12'].astype(int) # hodge number h12\n",
    "df['hashP'] = df['hashP'].astype(int) # points of MNP\n",
    "df['hashV'] = df['hashV'].astype(int) # vertices of MNP\n",
    "df['hashPbar'] = df['hashPbar'].astype(int) # points of dual MNP\n",
    "df['hashVbar'] = df['hashVbar'].astype(int) # vertices of dual MNP\n",
    "df['PI'] = pd.to_numeric(df['PI'], errors='coerce') # projections\n",
    "df['facet0'] = df['facet0'].astype(int) # first k3 facet\n",
    "df['facet1'] = pd.to_numeric(df['facet1'], errors='coerce') # second k3 facet\n",
    "df['facet2'] = pd.to_numeric(df['facet2'], errors='coerce') # third k3 facet\n",
    "# Numbers of K3 projections (#Pi) and K3 faecets\n",
    "\n",
    "# For removing unknown projections\n",
    "#predict = df[df['PI'].isna()]\n",
    "#df = df[df['PI'].notna()]\n",
    "#df = df[df['facet1'].notna()]\n",
    "#df = df[df['facet2'].notna()]\n",
    "\n",
    "# Filter rows based on the condition\n",
    "#df = df[df['h12'] <= 200]\n",
    "\n",
    "# convert facet1/2 to boolean\n",
    "#df['facet1'] = df['facet1'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "#df['facet2'] = df['facet2'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Create feature and target labels\n",
    "X = df.iloc[:, :5]\n",
    "y = np.array(df['PI'].values)\n",
    "\n",
    "# Choose a scaler: MinMaxScaler or StandardScaler\n",
    "#scaler = MinMaxScaler()  \n",
    "scaler = StandardScaler() # Best scaler from initial run\n",
    "\n",
    "# Fit the scaler to your DataFrame (only the columns you want to scale)\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform the columns using the fitted scaler\n",
    "scaled_X = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) # no scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.25, random_state=42) # x scaled only\n",
    "\n",
    "# Separate feature columns in the 'predict' DataFrame\n",
    "X_predict = predict.iloc[:, :5]\n",
    "\n",
    "# Transform the columns in the 'predict' DataFrame using the fitted scaler\n",
    "scaled_X_predict = pd.DataFrame(scaler.transform(X_predict), columns=X_predict.columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['h11'].unique()\n",
    "df['h11'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a35483",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df.describe(include='all')\n",
    "#print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df' and the column is called 'column_name'\n",
    "value_counts = df['PI'].value_counts(normalize=False)\n",
    "\n",
    "# Plot the frequencies as a bar chart\n",
    "value_counts.plot(kind='bar')\n",
    "plt.xlabel('Unique Values')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Frequency of Unique Values in Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225b07d",
   "metadata": {},
   "source": [
    "# Deep Learning w/ Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e4d5b",
   "metadata": {},
   "source": [
    "### Base Model - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep neural network model using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(96, activation='relu', input_shape=(5,) ))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.30))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.40))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0071962472189975694), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Train the model with the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cae62",
   "metadata": {},
   "source": [
    "### Base Model - Regression w/ Scaled Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77304b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep neural network model using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(192, activation='relu', input_shape=(5,) ))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(224, activation='relu'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.40))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Scale the target labels\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Train the model with the EarlyStopping callback and scaled target labels\n",
    "model.fit(X_train, y_train_scaled, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "loss, mae = model.evaluate(X_test, y_test_scaled, verbose=1)\n",
    "print(\"Scaled Mean Absolute Error:\", mae)\n",
    "\n",
    "# Convert the Scaled Mean Absolute Error back to the original scale\n",
    "mae_original_scale = y_scaler.inverse_transform(np.array([[mae]]))[0][0]\n",
    "print(\"Mean Absolute Error (Original Scale):\", mae_original_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3d445",
   "metadata": {},
   "source": [
    "### Base Model - Classification w/o Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34689997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a deep neural network model using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(192, activation='relu', input_shape=(5,) ))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(192, activation='relu'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(224, activation='relu'))\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(160, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(352, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Train the model with the EarlyStopping callback\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predicted probabilities for each class\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Calculate the F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Calculate the ROC AUC score for each class\n",
    "roc_auc_scores = roc_auc_score(y_test, y_pred_probs, multi_class='ovr', average='weighted')\n",
    "print(\"ROC AUC scores (weighted):\", roc_auc_scores)\n",
    "\n",
    "# Plot the ROC curve for each class\n",
    "n_classes = 4\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# Ensure y_test is an array of integers\n",
    "y_test_int = y_test.astype(int)\n",
    "\n",
    "# One-hot encode the true labels\n",
    "y_test_one_hot = np.eye(n_classes)[y_test_int]\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], y_pred_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "plt.figure()\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {i} (area = {roc_auc[i]:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a30d0b",
   "metadata": {},
   "source": [
    "### Base Model - Classification w/ Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e630dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep neural network model using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(192, activation='relu', input_shape=(5,)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(192, activation='relu'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(224, activation='relu'))\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(160, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(316, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes = np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model with the EarlyStopping callback and class weights\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping], class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model performance\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6223cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predicted probabilities for each class\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Calculate the F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Calculate the ROC AUC score for each class\n",
    "roc_auc_scores = roc_auc_score(y_test, y_pred_probs, multi_class='ovr', average='weighted')\n",
    "print(\"ROC AUC scores (weighted):\", roc_auc_scores)\n",
    "\n",
    "# Plot the ROC curve for each class\n",
    "n_classes = 4\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# Ensure y_test is an array of integers\n",
    "y_test_int = y_test.astype(int)\n",
    "\n",
    "# One-hot encode the true labels\n",
    "y_test_one_hot = np.eye(n_classes)[y_test_int]\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], y_pred_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "plt.figure()\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {i} (area = {roc_auc[i]:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071cb4b",
   "metadata": {},
   "source": [
    "### Base Model - Classification w/ Class Weights & One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def augment_data(X, y, num_permutations=10):\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        all_permutations = list(itertools.permutations(X[i]))\n",
    "        selected_permutations = random.sample(all_permutations, num_permutations)\n",
    "\n",
    "        X_augmented.extend(selected_permutations)\n",
    "        y_augmented.extend([y[i]] * num_permutations)\n",
    "\n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "X_train_augmented, y_train_augmented = augment_data(X_train.to_numpy(), y_train, num_permutations=10)\n",
    "unique_classes = np.unique(y_train)\n",
    "mapping_dict = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "\n",
    "y_train_mapped = np.array([mapping_dict[label] for label in y_train_augmented])\n",
    "y_test_mapped = np.array([mapping_dict[label] for label in y_test])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_augmented), y=y_train_augmented)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "y_train_encoded = to_categorical(y_train_mapped, num_classes=len(unique_classes))\n",
    "y_test_encoded = to_categorical(y_test_mapped, num_classes=len(unique_classes))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(192, activation='relu', input_shape=(5,)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(192, activation='relu'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(224, activation='relu'))\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(160, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(len(unique_classes), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_augmented, y_train_encoded, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping], class_weight=class_weights_dict)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded, verbose=1)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f809cc",
   "metadata": {},
   "source": [
    "# Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e61a0",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dffd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RegressionHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=256, step=32), \n",
    "                        activation='relu', input_shape=self.input_shape))\n",
    "        model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        for i in range(hp.Int('num_layers', min_value=1, max_value=3)):\n",
    "            model.add(Dense(units=hp.Int(f'units_{i+2}', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "            model.add(Dropout(hp.Float(f'dropout_{i+2}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "input_shape = (5,)\n",
    "hypermodel = RegressionHyperModel(input_shape)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    directory='random_search',\n",
    "    project_name='regression_final_h12'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Train the model with the EarlyStopping callback\n",
    "tuner.search(X_train, y_train, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "loss, mae = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5176bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best set of hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "best_hyperparameters.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36672f1c",
   "metadata": {},
   "source": [
    "### Classification w/o Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42dc90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class ClassificationHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=256, step=32),\n",
    "                        activation='relu', input_shape=self.input_shape))\n",
    "        model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        for i in range(hp.Int('num_layers', min_value=1, max_value=3)):\n",
    "            model.add(Dense(units=hp.Int(f'units_{i+2}', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "            model.add(Dropout(hp.Float(f'dropout_{i+2}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "input_shape = (5,)\n",
    "num_classes = 2\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "hypermodel = ClassificationHyperModel(input_shape, num_classes)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    directory='random_search',\n",
    "    project_name='classification_final_binary_facet1'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Train the model with the EarlyStopping callback\n",
    "tuner.search(X_train, y_train_encoded, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test_encoded, verbose=1)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best set of hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "best_hyperparameters.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae83de7",
   "metadata": {},
   "source": [
    "### Classification w/ Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class ClassificationHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=256, step=32),\n",
    "                        activation='relu', input_shape=self.input_shape))\n",
    "        model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        for i in range(hp.Int('num_layers', min_value=1, max_value=3)):\n",
    "            model.add(Dense(units=hp.Int(f'units_{i+2}', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "            model.add(Dropout(hp.Float(f'dropout_{i+2}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "input_shape = (5,)\n",
    "num_classes = 2\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "hypermodel = ClassificationHyperModel(input_shape, num_classes)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    directory='random_search',\n",
    "    project_name='classification_final_binary_facet2'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Compute the class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes = np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Train the model with the EarlyStopping callback and class weights\n",
    "tuner.search(X_train, y_train_encoded, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping], class_weight=class_weights)\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test_encoded, verbose=1)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best set of hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "best_hyperparameters.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5aab1",
   "metadata": {},
   "source": [
    "### Classification - Class Weights/One Hot Encoding/Label ReMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fedeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class ClassificationHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=256, step=32),\n",
    "                        activation='relu', input_shape=self.input_shape))\n",
    "        model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        for i in range(hp.Int('num_layers', min_value=1, max_value=3)):\n",
    "            model.add(Dense(units=hp.Int(f'units_{i+2}', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "            model.add(Dropout(hp.Float(f'dropout_{i+2}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'], weighted_metrics=['accuracy'])\n",
    "        #model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "input_shape = (5,)\n",
    "num_classes = 210\n",
    "\n",
    "# Create a mapping dictionary for class labels\n",
    "combined_y = np.concatenate((y_train, y_test))\n",
    "unique_classes = np.unique(combined_y)\n",
    "mapping_dict = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "\n",
    "# Map the original class labels to the new labels\n",
    "y_train_mapped = np.array([mapping_dict[label] for label in y_train])\n",
    "y_test_mapped = np.array([mapping_dict[label] for label in y_test])\n",
    "\n",
    "\n",
    "# Compute class weights for the new labels\n",
    "#class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "#class_weights_dict = {mapping_dict[label]: weight for label, weight in zip(np.unique(y_train), class_weights)}\n",
    "# Compute sample weights for the training set\n",
    "sample_weights = class_weight.compute_sample_weight('balanced', y_train_mapped)\n",
    "\n",
    "\n",
    "hypermodel = ClassificationHyperModel(input_shape, len(unique_classes))\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    directory='random_search',\n",
    "    project_name='classification_final_h12'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# One-hot encode the mapped labels\n",
    "y_train_encoded = to_categorical(y_train_mapped, num_classes=len(unique_classes))\n",
    "y_test_encoded = to_categorical(y_test_mapped, num_classes=len(unique_classes))\n",
    "\n",
    "# Train the model with the EarlyStopping callback and class weights\n",
    "#tuner.search(X_train, y_train_encoded, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping], class_weight=class_weights_dict)\n",
    "tuner.search(X_train, y_train_encoded, epochs=100, batch_size=64, validation_split=0.25, verbose=2, callbacks=[early_stopping], sample_weight=sample_weights)\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test_encoded, verbose=1)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8331de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best set of hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "best_hyperparameters.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377a825",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de400166",
   "metadata": {},
   "source": [
    "### Bagging - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec4ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_regression_model(callbacks=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(5,)))\n",
    "    model.add(Dropout(0.10))\n",
    "    model.add(Dense(224, activation='relu'))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.10))\n",
    "#    model.add(Dense(256, activation='relu'))\n",
    "#    model.add(Dropout(0.40))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001966340202965072), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "keras_regressor = KerasRegressor(build_fn=lambda: create_regression_model(callbacks=[early_stopping]), epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "n_estimators = 5  # Number of base models in the ensemble\n",
    "bagging_regressor = BaggingRegressor(estimator=keras_regressor, n_estimators=n_estimators, max_samples=0.8, random_state=42)\n",
    "\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bagging_regressor.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd8570",
   "metadata": {},
   "source": [
    "### Bagging - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b0878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_model(callbacks=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_shape=(5,)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(192, activation='relu'))\n",
    "    model.add(Dropout(0.10))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.20))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0007371287947368476), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "keras_model = KerasClassifier(build_fn=lambda: create_model(callbacks=[early_stopping]), epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "n_estimators = 5  # Number of base models in the ensemble\n",
    "bagging_model = BaggingClassifier(estimator=keras_model, n_estimators=n_estimators, max_samples=0.8, random_state=42)\n",
    "\n",
    "# Encode the labels if they are not already integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Fit the bagging model\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "accuracy = bagging_model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b836f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Obtain predicted probabilities for each class\n",
    "y_pred_probs = bagging_model.predict_proba(X_test)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Calculate the F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.4)  # Increase font size\n",
    "ax = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False, annot_kws={\"size\": 14})\n",
    "plt.xlabel('Predicted Label', fontsize=16)\n",
    "plt.ylabel('True Label', fontsize=16)\n",
    "plt.title(\"Confusion Matrix\", fontsize=18)\n",
    "\n",
    "# Customize tick labels\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=14)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=14)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate the ROC AUC score for each class\n",
    "#roc_auc_scores = roc_auc_score(y_test, y_pred_probs, multi_class='ovr', average='weighted')\n",
    "#print(\"ROC AUC scores (weighted):\", roc_auc_scores)\n",
    "\n",
    "# Plot the ROC curve for each class\n",
    "n_classes = 4\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# Ensure y_test is an array of integers\n",
    "y_test_int = y_test.astype(int)\n",
    "\n",
    "# One-hot encode the true labels\n",
    "y_test_one_hot = np.eye(n_classes)[y_test_int]\n",
    "\n",
    "# Calculate the ROC AUC score for each class\n",
    "roc_auc_scores = roc_auc_score(y_test_one_hot, y_pred_probs, average='weighted')\n",
    "print(\"ROC AUC scores (weighted):\", roc_auc_scores)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], y_pred_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "plt.figure()\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {i} (area = {roc_auc[i]:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"k3_fibrations_classification_proj_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78119167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the bagging model\n",
    "predictions = bagging_model.predict(scaled_X_predict)\n",
    "\n",
    "# Obtain class probabilities using the bagging model\n",
    "probabilities = bagging_model.predict_proba(scaled_X_predict)\n",
    "\n",
    "# Calculate the confidence for each prediction\n",
    "confidence = np.max(probabilities, axis=1)\n",
    "\n",
    "# Convert predictions to integer type\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "# Inverse transform the predicted labels\n",
    "predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Combine the original X_predict features with the predicted labels and confidence\n",
    "predicted_df = X_predict.copy()\n",
    "predicted_df['Predicted_PI'] = predicted_labels\n",
    "predicted_df['Confidence'] = confidence\n",
    "\n",
    "print(predicted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = predicted_df['Predicted_PI'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
